{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28712d66ca25b58f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Key Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa89155606dc33",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "Welcome to `braintrace`!\n",
    "\n",
    "BrainTrace is a Python library designed for implementing online learning in neural network models with dynamics. Online learning represents a learning paradigm that enables continuous parameter updates as models receive new data streams. This approach proves particularly valuable in real-world applications, including robotic control systems, agent decision-making processes, and large-scale data stream processing.\n",
    "\n",
    "\n",
    "In this section, I will introduce some of the key concepts that are fundamental to understanding and using online learning methods defined in ``braintrace`` . These concepts include:\n",
    "\n",
    "- Concepts related to build high-Level Neural Networks.\n",
    "- Concepts related to customize neural network module: ``ETraceVar`` for hidden states, ``ETraceParam`` for weight parameters, and ``ETraceOp`` for input-to-hidden transition.\n",
    "- Concepts for online learning algorithms ``ETraceAlgorithm``.\n",
    "\n",
    "``braintrace`` is seamlessly integrated in the [brain dynamics programming ecosystem](https://brainmodeling.readthedocs.io/) centred on ``brainstate``. We strongly recommend that you first familiarise yourself with [basic usage of ``brainstate``](https://brainstate.readthedocs.io/), as this will help you better understand how ``braintrace`` works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848fc7b2e198e56f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:34:56.265372Z",
     "start_time": "2025-07-21T08:34:54.980700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import brainstate \n",
    "import brainunit as u\n",
    "import braintrace\n",
    "import braintools\n",
    "import brainpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b588265bf6265",
   "metadata": {},
   "source": [
    "## 1. Dynamical Models Supported in ``braintrace``\n",
    "\n",
    "``BrainTrace`` does not support online learning for arbitrary dynamical models. The dynamical models currently supported by ``BrainTrace`` exhibit a specific architectural constraint, as illustrated in the figure below, wherein the \"dynamics\" and \"interactions between dynamics\" are strictly segregated. Models adhering to this architecture can be decomposed into two primary components:\n",
    "\n",
    "- **Dynamics**: This component characterizes the intrinsic dynamics of neurons, encompassing models such as the Leaky Integrate-and-Fire (LIF) neuron model, the FitzHugh-Nagumo model, and Long Short-Term Memory (LSTM) networks. The update of dynamics (hidden states) is implemented through strictly element-wise operations, although the model may incorporate multiple hidden states.\n",
    "\n",
    "- **Interaction between Dynamics**: This component defines the interactions between neurons, implemented through weight matrices or connectivity matrices. The interactions between model dynamics can be realized through standard matrix multiplication, convolutional operations, or sparse operations.\n",
    "\n",
    "![](../_static/model-dynamics-supported.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36152a55249cdebd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To elucidate the class of dynamical models supported by BrainTrace, let us examine a fundamental Leaky Integrate-and-Fire (LIF) neural network model. The dynamics of this network are governed by the following differential equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tau \\frac{dv_i}{dt} &= -v_i + I_{\\text{ext}} + s_i \\\\\n",
    "\\tau_s \\frac{ds_i}{dt} &= -s_i + \\sum_{j} w_{ij} \\delta(t - t_j)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here, $v_i$ represents the membrane potential of the neuron. When this potential exceeds a threshold value $v_{th}$, the neuron generates an action potential and its membrane potential is reset to $v_{\\text{reset}}$, as described by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_i & = \\mathcal{H}(v_i-v_{th}) \\\\\n",
    "v_i & \\leftarrow v_{\\text{reset}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Additionally, $s_i$ denotes the postsynaptic current, $I_{\\text{ext}}$ represents the external input current, $w_{ij}$ is the synaptic weight from neuron $i$ to neuron $j$, and $\\delta(t - t_j)$ is the Dirac delta function indicating the reception of a synaptic event at time $t_j$. The time constants $\\tau$ and $\\tau_s$ characterize the temporal evolution of the membrane potential and postsynaptic current, respectively.\n",
    "\n",
    "Through numerical integration, we discretize the above differential equations and express them in vector form, yielding the following update rules:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{v}_i^{t+1} &= \\mathbf{v}_i^{t} + \\frac{\\Delta t}{\\tau} (-\\mathbf{v}_i^{t} + \\mathbf{I}_{\\text{ext}} + \\mathbf{s}^t) \\\\\n",
    "\\mathbf{s}_i^{t+1} &= \\mathbf{s}_i^{t} + \\frac{\\Delta t}{\\tau_s} (-\\mathbf{s}_i^{t} + \\underbrace{  W \\mathbf{z}^t  } _ {\\text{neuronal interaction}} )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notably, the dynamics of the LIF neurons are updated through element-wise operations, while the interaction component is implemented via matrix multiplication. All dynamical models supported by BrainTrace can be decomposed into similar `dynamics` and `interaction` components. It is particularly worth noting that this architecture encompasses the majority of recurrent neural network models, thus enabling BrainTrace to support online learning for a wide range of recurrent neural network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146839ccad9ff46",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. `braintrace.nn`: Constructing Neural Networks with Online Learning Support\n",
    "\n",
    "The construction of neural network models supporting online learning in BrainTrace follows identical conventions as those employed in `brainstate`. For comprehensive tutorials, please refer to the documentation on [Building Artificial Neural Networks](https://brainstate.readthedocs.io/tutorials/examples/11_artificial_neural_networks.html) and [Simulating Spiking Neural Networks](https://brainstate.readthedocs.io/tutorials/examples/13_snn_simulation.html).\n",
    "\n",
    "The sole distinction lies in the requirement to utilize components from the [`braintrace.nn` module](../apis/nn.rst) for model construction. These components represent extensions of `brainstate.nn` module, specifically engineered to provide modular units with online learning capabilities.\n",
    "\n",
    "Below, we present a basic implementation demonstrating the construction of a Leaky Integrate-and-Fire (LIF) neural network using the `braintrace.nn` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298e843a447e8e6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:34:56.336168Z",
     "start_time": "2025-07-21T08:34:56.331129Z"
    }
   },
   "outputs": [],
   "source": [
    "class LIF_Delta_Net(brainstate.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in,\n",
    "        n_rec,\n",
    "        tau_mem=5. * u.ms,\n",
    "        V_th=1. * u.mV,\n",
    "        spk_fun=braintools.surrogate.ReluGrad(),\n",
    "        spk_reset: str = 'soft',\n",
    "        rec_scale: float = 1.,\n",
    "        ff_scale: float = 1.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Using the LIF model in braintrace.nn\n",
    "        self.neu = brainpy.state.LIF(n_rec, tau=tau_mem, spk_fun=spk_fun, spk_reset=spk_reset, V_th=V_th)\n",
    "\n",
    "        # Constructing input and recurrent connection weights\n",
    "        rec_init = braintools.init.KaimingNormal(rec_scale, unit=u.mV)\n",
    "        ff_init = braintools.init.KaimingNormal(ff_scale, unit=u.mV)\n",
    "        w_init = u.math.concatenate([ff_init([n_in, n_rec]), rec_init([n_rec, n_rec])], axis=0)\n",
    "\n",
    "        # Using delta synaptic projections to construct input and recurrent connections\n",
    "        self.syn = brainpy.state.DeltaProj(\n",
    "            # Using the Linear model in braintrace.nn\n",
    "            comm=braintrace.nn.Linear(n_in + n_rec, n_rec, w_init=w_init, b_init=braintools.init.ZeroInit(unit=u.mV)),\n",
    "            post=self.neu\n",
    "        )\n",
    "\n",
    "    def update(self, spk):\n",
    "        inp = u.math.concatenate([spk, self.neu.get_spike()], axis=-1)\n",
    "        self.syn(inp)\n",
    "        self.neu()\n",
    "        return self.neu.get_spike()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffc1c4945fa7cf2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this exemplar implementation, we define a `LIF_Delta_Net` class that inherits from `brainstate.nn.Module`. The architecture incorporates two primary components: a Leaky Integrate-and-Fire (LIF) neuron model implemented as `self.neu`, and a `DeltaProj` module designated as `self.syn` which manages both input and recurrent connectivity.\n",
    "\n",
    "Subsequently, we shall proceed to construct a three-layer Gated Recurrent Unit (GRU) neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f482d8336349bd33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:34:56.361170Z",
     "start_time": "2025-07-21T08:34:56.357659Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRU_Net(brainstate.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in: int,\n",
    "        n_rec: int,\n",
    "        n_out: int,\n",
    "        n_layer: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Building the GRU Layer\n",
    "        self.layers = []\n",
    "        for i in range(n_layer - 1):\n",
    "            # Using the GRUCell model within braintrace.nn\n",
    "            self.layers.append(braintrace.nn.GRUCell(n_in, n_rec))\n",
    "            n_in = n_rec\n",
    "        self.layers.append(braintrace.nn.GRUCell(n_in, n_out))\n",
    "\n",
    "    def update(self, x):\n",
    "        # Updating the GRU Layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4dbf7a96571156",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As demonstrated, the process of constructing neural network models using the [`braintrace.nn` module](../apis/nn.rst) maintains complete procedural equivalence with the construction methodology employed in the [`brainstate.nn` module](https://brainstate.readthedocs.io/apis/nn.html). This architectural parallelism enables direct utilization of existing `brainstate` tutorials for developing neural network models with online learning capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb5d8ec76b1ee3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. `ETraceState`, `ETraceParam`, and `ETraceOp`: Customizing Network Modules\n",
    "\n",
    "While the `braintrace.nn` module provides fundamental network components, it does not encompass all possible network dynamics. Consequently, BrainTrace implements a mechanism for customizing module development through three primary classes: `ETraceState`, `ETraceParam`, and `ETraceOp`.\n",
    "\n",
    "**Core Components**\n",
    "\n",
    "- **`brainstate.HiddenState`**: Represents the hidden states $\\mathbf{h}$ within modules, defining dynamical states such as membrane potentials in LIF neurons or postsynaptic conductances in exponential synaptic models.\n",
    "\n",
    "- **`braintrace.ETraceOp`**: Describe network connections, or how input data is used to compute postsynaptic currents based on model parameters, such as linear matrix multiplication, sparse matrix multiplication, and convolution operations.\n",
    "\n",
    "- **`braintrace.ETraceParam`**: Corresponds to model parameters $\\theta$ within modules, encompassing elements such as weight matrices for linear matrix multiplication and adaptive time constants in LIF neurons. All parameters requiring gradient updates during training must be defined within `ETraceParam`.\n",
    "\n",
    "\n",
    "**Foundational Framework**\n",
    "\n",
    "These three components—`ETraceState`, `ETraceParam`, and `ETraceOp`—constitute the fundamental conceptual framework underlying neural network models with online learning capabilities in BrainTrace.\n",
    "\n",
    "In the following sections, we will present a series of illustrative examples demonstrating the practical implementation of custom network modules using `ETraceState`, `ETraceParam`, and `ETraceOp`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89a04bcad72713d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1 `ETraceState`: Model State\n",
    "\n",
    "Let us first examine a fundamental Leaky Integrate-and-Fire (LIF) neuron model, whose dynamics are characterized by the following differential equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tau \\frac{dv_i}{dt} &= -v_i + I_{\\text{ext}} + v_\\text{rest} \\\\\n",
    "z_i & = \\mathcal{H}(v_i-v_{th}) \\\\\n",
    "v_i & \\leftarrow v_{\\text{reset}} \\quad \\text{if} \\quad z_i > 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here, $v_i$ represents the membrane potential of the neuron. When this potential exceeds a threshold value $v_{th}$, the neuron generates an action potential, and its membrane potential is reset to $v_{\\text{reset}}$. The Heaviside function $\\mathcal{H}$ characterizes the action potential generation, while $I_{\\text{ext}}$ denotes the external input current. The temporal evolution of the membrane potential is governed by the time constant $\\tau$, and $v_\\text{rest}$ represents the resting membrane potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b5b27ce681d400",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:34:56.374942Z",
     "start_time": "2025-07-21T08:34:56.369243Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class LIF(brainpy.state.Neuron):\n",
    "    \"\"\"\n",
    "    Leaky integrate-and-fire neuron model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: brainstate.typing.Size,\n",
    "        R: brainstate.typing.ArrayLike = 1. * u.ohm,\n",
    "        tau: brainstate.typing.ArrayLike = 5. * u.ms,\n",
    "        V_th: brainstate.typing.ArrayLike = 1. * u.mV,\n",
    "        V_reset: brainstate.typing.ArrayLike = 0. * u.mV,\n",
    "        V_rest: brainstate.typing.ArrayLike = 0. * u.mV,\n",
    "        V_initializer: Callable = braintools.init.Constant(0. * u.mV),\n",
    "        spk_fun: Callable = braintools.surrogate.ReluGrad(),\n",
    "        spk_reset: str = 'soft',\n",
    "        name: str = None,\n",
    "    ):\n",
    "        super().__init__(size, name=name, spk_fun=spk_fun, spk_reset=spk_reset)\n",
    "\n",
    "        # parameters\n",
    "        self.R = braintools.init.param(R, self.varshape)\n",
    "        self.tau = braintools.init.param(tau, self.varshape)\n",
    "        self.V_th = braintools.init.param(V_th, self.varshape)\n",
    "        self.V_rest = braintools.init.param(V_rest, self.varshape)\n",
    "        self.V_reset = braintools.init.param(V_reset, self.varshape)\n",
    "        self.V_initializer = V_initializer\n",
    "\n",
    "    def init_state(self, batch_size: int = None, **kwargs):\n",
    "        # Here is the most critical step, we define an ETraceState class \n",
    "        # that describes the kinetic state of membrane potentials\n",
    "        self.V = brainstate.HiddenState(\n",
    "            braintools.init.param(self.V_initializer, self.varshape, batch_size))\n",
    "\n",
    "    def reset_state(self, batch_size: int = None, **kwargs):\n",
    "        self.V.value = braintools.init.param(self.V_initializer, self.varshape, batch_size)\n",
    "\n",
    "    def get_spike(self, V=None):\n",
    "        V = self.V.value if V is None else V\n",
    "        v_scaled = (V - self.V_th) / (self.V_th - self.V_reset)\n",
    "        return self.spk_fun(v_scaled)\n",
    "\n",
    "    def update(self, x=0. * u.mA):\n",
    "        last_v = self.V.value\n",
    "        lst_spk = self.get_spike(last_v)\n",
    "        V_th = self.V_th if self.spk_reset == 'soft' else jax.lax.stop_gradient(last_v)\n",
    "        V = last_v - (V_th - self.V_reset) * lst_spk\n",
    "        # membrane potential\n",
    "        dv = lambda v: (-v + self.V_rest + self.R * self.sum_current_inputs(x, v)) / self.tau\n",
    "        V = brainstate.nn.exp_euler_step(dv, V)\n",
    "        V = self.sum_delta_inputs(V)\n",
    "        self.V.value = V\n",
    "        return self.get_spike(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349531d8061ba2c3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the code above, we implement the `LIF` model through inheritance from `brainpy.state.Neuron`. The class incorporates an `ETraceState` class variable `self.V` that characterizes the dynamical state of the membrane potential. The `init_state` method establishes the initial conditions for the membrane potential dynamics, while the `update` method implements the temporal evolution of these dynamics.\n",
    "\n",
    "This implementation maintains substantial structural similarity with the `LIF` class definition in `brainstate`, with one crucial distinction: whereas `brainstate` employs `brainstate.HiddenState` to represent the membrane potential dynamics, `braintrace` utilizes `brainstate.ETraceState` to explicitly designate this dynamical state for online learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc51aa04614f01",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Therefore, we say that `brainstate.HiddenState` can be conceptualized as the counterpart to `brainstate.HiddenState`, specifically designed for defining model states that require eligibility trace updates. Should model states be defined using `brainstate.HiddenState` rather than `brainstate.HiddenState`, the online learning compiler in `braintrace` will fail to recognize these states. This oversight results in the compiled online learning rules being ineffective for the affected states, potentially leading to erroneous or omitted gradient updates in the model.\n",
    "\n",
    "But we should still be aware that there are obvious differences between `ETraceState` and `HiddenState`:\n",
    "- `brainstate.HiddenState`: Explicitly marks states for eligibility trace computation\n",
    "- `brainstate.HiddenState`: Standard state representation within ``branstate`` without online learning capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7478130b83dccc0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 `ETraceOp`: Model Connectivity\n",
    "\n",
    "`ETraceOp` is a conceptual abstraction for describing model connectivity or how different dynamical components interact. It defines how model inputs are transformed into outputs based on specific parameters.\n",
    "\n",
    "\n",
    "The standard usage format is:\n",
    "\n",
    "```python\n",
    "y = op(x, param)\n",
    "```\n",
    "\n",
    "Here, `x` represents the input data, `param` denotes the model parameters, and `y` is the resulting output. The core functionality of `ETraceOp` lies in applying a specific connection rule to combine the input data and parameters in order to compute the output.\n",
    "\n",
    "\n",
    "\n",
    "BrainTrace provides several built-in operators for common modeling operations, including linear matrix multiplication, sparse matrix multiplication, and convolution. These include:\n",
    "\n",
    "* [`braintrace.MatMulOp`](../apis/generated/braintrace.MatMulOp.rst): Standard matrix multiplication.\n",
    "* [`braintrace.ConvOp`](../apis/generated/braintrace.ConvOp.rst): Standard convolution operation.\n",
    "* [`braintrace.SpMatMulOp`](../apis/generated/braintrace.SpMatMulOp.rst): Sparse matrix-vector multiplication.\n",
    "* [`braintrace.LoraOp`](../apis/generated/braintrace.LoraOp.rst): Low-Rank Adaptation (LoRA) operation.\n",
    "* [`braintrace.ElemWiseOp`](../apis/generated/braintrace.ElemWiseOp.rst): Element-wise operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef04a53abc80dd6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 `ETraceParam` : Model Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc068e8",
   "metadata": {},
   "source": [
    "`ETraceParam` in BrainTrace is used to define trainable model parameters. It takes the following form:\n",
    "\n",
    "```python\n",
    "param = braintrace.ETraceParam(parameters, op)\n",
    "```\n",
    "\n",
    "Here, `parameters` refers to the model parameters, and `op` is an instantiated `ETraceOp`. The typical usage pattern is:\n",
    "\n",
    "```python\n",
    "y = param.execute(x)\n",
    "```\n",
    "\n",
    "Below, we use linear matrix multiplication as an example to demonstrate how to define a weight matrix and bias vector for a linear layer using `ETraceParam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce368fdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:34:56.392072Z",
     "start_time": "2025-07-21T08:34:56.388954Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_weight(\n",
    "    n_in, n_out, init: Callable = braintools.init.KaimingNormal()\n",
    ") -> braintrace.ETraceParam:\n",
    "    weight = init([n_in, n_out])\n",
    "    bias = braintools.init.ZeroInit()([n_out])\n",
    "    \n",
    "    # Here is the most crucial step, we define an ETraceParam class to describe the weight matrix and bias vector\n",
    "    return braintrace.ETraceParam(\n",
    "        {'weight': weight, 'bias': bias},  # model parameters\n",
    "        braintrace.MatMulOp()  # operation\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22a4a05",
   "metadata": {},
   "source": [
    "In the code above, we define a `generate_weight` function that produces weight matrices and bias vectors. This function returns an `ETraceParam` object that encapsulates these parameters.\n",
    "\n",
    "`braintrace.ETraceParam` serves as the counterpart to `brainstate.ParamState`, specifically designed for model parameters requiring eligibility trace updates. When model parameters $\\theta$ are defined using `braintrace.ETraceParam`, the online learning compiler in `braintrace` implements temporally-dependent gradient updates according to the following formula:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathcal{L}=\\sum_{t} \\frac{\\partial \\mathcal{L}^{t}}{\\partial \\mathbf{h}^{t}} \\sum_{k=1}^t \\frac{\\partial \\mathbf{h}^t}{\\partial \\boldsymbol{\\theta}^k},\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\theta}^k$ represents the weight $\\boldsymbol{\\theta}$ utilized at time step $k$.\n",
    "\n",
    "Conversely, when model parameters $\\theta$ are defined using `brainstate.ParamState`, the online learning compiler computes only the instantaneous gradient of the loss function with respect to the weights:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathcal{L}=\\sum_{t} \\frac{\\partial \\mathcal{L}^{t}}{\\partial \\mathbf{h}^{t}} \\frac{\\partial \\mathbf{h}^t}{\\partial \\boldsymbol{\\theta}^t}.\n",
    "$$\n",
    "\n",
    "This implementation distinction signifies that in `braintrace`'s online learning framework, parameters defined as `brainstate.ParamState` are treated as those not requiring eligibility trace updates, thereby forfeiting the ability to compute gradients with temporal dependencies. This architectural design enhances the flexibility of parameter update patterns, thereby increasing the customizability of gradient computation mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f1dcc",
   "metadata": {},
   "source": [
    "\n",
    "Besides, `ETraceParam` encapsulates both model parameters and their associated operations, providing a comprehensive framework for describing parameter-operation combinations. During instantiation, it requires both a parameter object and an operation function as inputs.\n",
    "\n",
    "Using the `ETraceParam` framework, we can reformulate the matrix multiplication operator discussed above as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43454dfe8ceaa4c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:34:56.408743Z",
     "start_time": "2025-07-21T08:34:56.403901Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Linear(brainstate.nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_size: brainstate.typing.Size,\n",
    "        out_size: brainstate.typing.Size,\n",
    "        w_init: Callable = braintools.init.KaimingNormal(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # input and output shape\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "        # weights\n",
    "        weight = braintools.init.param(w_init, [self.in_size[-1], self.out_size[-1]], allow_none=False)\n",
    "        \n",
    "        # operation\n",
    "        op = braintrace.MatMulOp()\n",
    "        \n",
    "        # Here is the most crucial step, we define an ETraceParam class to describe the weight matrix and operations\n",
    "        self.weight_op = braintrace.ETraceParam(weight, op)\n",
    "\n",
    "    def update(self, x):\n",
    "        # Operation of ETraceParam\n",
    "        return self.weight_op.execute(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734fdf3d1fbdf5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As demonstrated in the code above, `ETraceParam` represents an integrated construct that unifies model parameters and their associated operations. This unified framework requires both a parameter object and an operation function for instantiation. The `execute` method of `ETraceParam` implements the operational transformation, converting input data into output data according to the specified parameters and operations.\n",
    "\n",
    "Through this implementation, we have successfully defined a fundamental `Linear` layer module, encompassing a weight matrix and its corresponding matrix multiplication operation. This module serves as a building block for constructing neural network models with online learning capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76668ec1ab34eaeb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. `ETraceAlgorithm`: Online Learning Algorithms\n",
    "\n",
    "`ETraceAlgorithm` represents another fundamental concept in the BrainTrace framework, defining both the eligibility trace update process during model state evolution and the gradient update rules for model parameters. Implemented as an abstract class, `ETraceAlgorithm` serves as a specialized framework for describing various forms of online learning algorithms within BrainTrace.\n",
    "\n",
    "The algorithmic support provided by `braintrace.ETraceAlgorithm` is founded upon the three fundamental concepts previously introduced: `ETraceState`, `ETraceParam`, and `ETraceOp`. \n",
    "\n",
    "`braintrace.ETraceAlgorithm` implements a flexible online learning compiler that enables online learning capabilities for any neural network model constructed using these three foundational concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49bb176c385eb1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Specifically, BrainTrace currently supports the following online learning algorithms:\n",
    "\n",
    "1. [`braintrace.IODimVjpAlgorithm`](../apis/generated/braintrace.IODimVjpAlgorithm.rst) or [`braintrace.ES_D_RTRL`](../apis/generated/braintrace.ES_D_RTRL.rst)\n",
    "    - Implements the ES-D-RTRL algorithm for online learning\n",
    "    - Achieves $O(N)$ memory and computational complexity for online gradient computation\n",
    "    - Optimized for large-scale spiking neural network models\n",
    "    - Detailed algorithm specifications are available in [our paper](https://doi.org/10.1101/2024.09.24.614728)\n",
    "\n",
    "2. [`braintrace.ParamDimVjpAlgorithm`](../apis/generated/braintrace.ParamDimVjpAlgorithm.rst) or [`braintrace.D_RTRL`](../apis/generated/braintrace.D_RTRL.rst)\n",
    "    - Utilizes the D-RTRL algorithm for online learning\n",
    "    - Features $O(N^2)$ memory and computational complexity for online gradient computation\n",
    "    - Applicable to both recurrent neural networks and spiking neural network models\n",
    "    - Complete algorithmic details are documented in [our paper](https://doi.org/10.1101/2024.09.24.614728)\n",
    "\n",
    "3. [`braintrace.HybridDimVjpAlgorithm`](../apis/generated/braintrace.HybridDimVjpAlgorithm.rst)\n",
    "    - Implements selective application of ES-D-RTRL or D-RTRL algorithms for parameter updates\n",
    "    - Preferentially employs D-RTRL for convolutional layers and highly sparse connections\n",
    "    - Optimizes memory and computational complexity of parameter updates through adaptive algorithm selection\n",
    "\n",
    "The framework is designed for extensibility, with ongoing development to support additional online learning algorithms for diverse application scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6bd8f6a4fb35dd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the following demonstration, we will illustrate the process of constructing a neural network model with online learning capabilities using `braintrace.ETraceAlgorithm`. This example will serve to exemplify the practical implementation of the concepts discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c39ae571c630da9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:34:57.112715Z",
     "start_time": "2025-07-21T08:34:56.425875Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with brainstate.environ.context(dt=0.1 * u.ms):\n",
    "\n",
    "    # Define a simple recurrent neural network composed of LIF neurons\n",
    "    model = LIF_Delta_Net(10, 10)\n",
    "    brainstate.nn.init_all_states(model)\n",
    "    \n",
    "    # The model is fed into an online learning algorithm with a view to online learning\n",
    "    model = braintrace.IODimVjpAlgorithm(model, decay_or_rank=0.99)\n",
    "    \n",
    "    # Compile the model's eligibility trace based on one input data. \n",
    "    # Thereafter, the model is called to update not only the state \n",
    "    # of the model, but also the model's eligibility trace\n",
    "    example_input = brainstate.random.random(10) < 0.1\n",
    "    model.compile_graph(example_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f244553c077d8aba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In essence, the user-defined neural network model solely specifies how the model states $\\mathbf{h}$ evolve forward in time as a function of inputs. The compiled `ETraceAlgorithm`, in contrast, defines the update dynamics of the model's eligibility traces $\\mathbf{\\epsilon}$ in relation to state updates. Consequently, subsequent model invocations result in concurrent updates of both model states and their corresponding eligibility traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e050256b605e9a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:34:57.930134Z",
     "start_time": "2025-07-21T08:34:57.137265Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  2152391028160: EligibilityTrace(\n",
       "    value=ShapedArray(float32[20])\n",
       "  )\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with brainstate.environ.context(dt=0.1 * u.ms):\n",
    "    \n",
    "    out = model(example_input)\n",
    "\n",
    "# The eligibility trace of the model's pre-synaptic neural \n",
    "# activity trace can be obtained by calling \"model.etrace_xs\"\n",
    "brainstate.util.PrettyMapping(model.etrace_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c10f9de21c6938",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:34:57.950011Z",
     "start_time": "2025-07-21T08:34:57.945268Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  (2152391028288, 'hidden_group_0'): EligibilityTrace(\n",
       "    value=ShapedArray(float32[10,1])\n",
       "  )\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The eligibility trace of the model's post-synaptic neural \n",
    "# activity trace can be obtained by calling \"model.etrace_dfs\"\n",
    "brainstate.util.PrettyMapping(model.etrace_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ab12e085b644b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "`BrainTrace` provides a comprehensive and elegant framework for online learning, with its core concepts organized into the following hierarchical layers:\n",
    "\n",
    "1. Infrastructure Layer\n",
    "    - Supports specific dynamical model architectures with strict separation between \"dynamics\" and \"interactions\"\n",
    "    - Built upon the `BrainState` ecosystem, maintaining full compatibility with its programming paradigm\n",
    "    - Provides ready-to-use neural network components through the `braintrace.nn` module\n",
    "\n",
    "2. Core Concepts Layer\n",
    "    - `ETraceState`: Designates hidden states requiring eligibility trace updates\n",
    "    - `ETraceOp`: Defines specific operations for dynamical interactions\n",
    "    - `ETraceParam`: Provides unified interface for parameter-operation combinations\n",
    "\n",
    "3. Algorithm Implementation Layer\n",
    "    - `DiagIODimAlgorithm`: Implements ES-D-RTRL algorithm with $O(N)$ complexity\n",
    "    - `DiagParamDimAlgorithm`: Implements D-RTRL algorithm with $O(N^2)$ complexity\n",
    "    - `DiagHybridDimAlgorithm`: Adaptive hybrid approach, selecting between $O(N)$ and $O(N^2)$ complexity algorithms based on network architecture\n",
    "\n",
    "4. Operational Workflow\n",
    "    - Construction of neural networks using foundational components\n",
    "    - Selection and application of appropriate online learning algorithms\n",
    "    - Model compilation generating eligibility trace computation graphs\n",
    "    - Concurrent updates of model states and eligibility traces through forward propagation\n",
    "\n",
    "BrainTrace's distinctive architecture encapsulates complex online learning algorithms behind concise interfaces while providing flexible customization mechanisms. This design philosophy achieves a balance between:\n",
    "- High performance and usability\n",
    "- Seamless integration with the existing BrainState ecosystem\n",
    "- Intuitive and efficient construction of online learning neural networks\n",
    "\n",
    "Through this architectural approach, BrainTrace transforms the development and training of online learning neural networks into an intuitive and efficient process, providing a powerful toolkit for neural computation research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ecosystem-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
